---
format:
  pdf:
    number-sections: true
lang: es    
execute:
  include: false
  fig-align: center
---

```{r}
#| label: setup

pacman::p_load(dplyr, ggplot2, rsample, caret, RColorBrewer, knitr, glmnet, pls, rpart.plot, gbm,tibble, rpart, rpart.plot, here)
seed <- 1234
doParallel::registerDoParallel()
vessel_x <- read.table(here("Taller Datos","datos", "Vessel_X.txt"),sep=",")
vessel_y <- read.table(here("Taller Datos","datos", "Vessel_Y.txt"),sep=",")
df <- cbind(vessel_y[,6],vessel_x)
names(df)[1] <- "comp_6"
```

\thispagestyle{empty}

![](exactas_uba.png){width=150 fig-align="center"}

\begin{center}
    \Large{Universidad de Buenos Aires \\
    Facultad de Ciencias Exactas y Naturales} \\
    \large{Departamento de Matemática e Instituto de Cálculo}
\end{center}

\vspace{10pt}

\begin{center}
    \Huge\textbf{Trabajo Final Integrador}
\end{center}

\begin{center}
    \Large{Martín Gabriel Cargnel}
\end{center}

\vfill

\begin{flushleft}
    \large{Profesor: Dr. Ricardo Maronna}
\end{flushleft}

\begin{center}
    \Large{Febrero, 2024}
\end{center}

{{< pagebreak >}}

# Introducción

Para determinar la composición de un conjunto de vasijas de vidrio de un yacimiento arqueológico se puede optar por dos alternativas: llevar a cabo un análisis químico ó calibrar un modelo que use los datos de una espectometría. Siendo el primero más caro que el segundo, es de interés encontrar un modelo que nos permita reemplazar el análisis químico. Por lo cual, en este trabajo se compara la *performance* predictiva de distintos modelos para encontrar el que mejor prediga el contenido de uno de los compuestos químicos de las vasijas.

Los datos con los que se realiza este análisis son una muestra de `r nrow(df)` vasijas a las que se les realizó tanto una espectometría de rayos X sobre 1920 frecuencias, como un análisis de laboratorio para determinar el contenido de 13 compuestos químicos: $$Na_2O, \; MgO, \; Al_2O_3, \; SiO_2, \; P_2O_5, \; SO_3, \; Cl, \; K_2O, \; CaO, \; MnO, \; Fe_2O_3, \; BaO \; y \; PbO$$

Para este estudio sólo se tendrán en cuenta las frecuencias 100 a 400 (por ser las únicas que no tienen valores casi nulos) y se buscará predecir el compuesto $SO_3$. Cabe aclarar que cada covariable del modelo será la energía correspondiente a cada frecuencia.

Este trabajo fue realizado en su totalidad con el *software* estadístico R, se utilizaron los libros detallados en la sección de referencias como guía metodológica y se estructura de la siguiente forma: en la @sec-analisis-exploratorio se lleva a cabo un análisis exploratorio de los datos con los que se va a trabajar. Luego se presentan e implementan distintos modelos en la @sec-modelos, para después comparar su poder predictivo en términos del error de predicción en la @sec-comp-resultados. Una vez identificado el modelo que tenga el error de predicción más bajo se procede a estudiar en profundidad las predicciones que realiza en la @sec-supuestos. Finalmente, el trabajo concluye en la @sec-conclusiones y en el apéndice se pueden ver en detalle los resultados de cada modelo.

# Análisis Exploratorio {#sec-analisis-exploratorio}

En primer lugar se realizó un análisis exploratorio de los datos con los que se cuenta. Por lo que se graficó la media de cada espectro de frecuencia y la varianza, asi como también la covarianza que cada frecuencia tiene con el compuesto $SO_3$ que se quiere predecir. Se puede observar en las @fig-med , @fig-var y @fig-covs que no todas las frecuencias son igual de relevantes *a priori*, ya que no todas tienen ni mucha media o varianza, ni tampoco correlacionan mucho con la variable de respuesta. Hay rangos de frecuencias, como los que están alrededor de la frecuencia 70, 220 o 260, que condensan la mayor cantidad de "*información*" del compuesto. Más adelante se valida si dichas frecuencias son las que el modelo que mejor prediga toma como más relevante.

```{r}
#| label: summary-data

summary_x <- data.frame(frecuencias=seq(1,301),
                  media=sapply(df[,-1], mean),
                  varianza = sapply(df[,-1], var))
```

```{r}
#| label: fig-med
#| fig-cap: Media de la enegía del espectro correspondiente a las frecuencias.
#| echo: false
#| include: true

summary_x %>% ggplot(aes(x=frecuencias, y=media)) +
  geom_point(color="#2b8cbe") +
  labs(title="Medias por frecuencia", x ="Frecuencia", y = "Medias") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
#| label: fig-var
#| fig-cap: Varianza de la energía del espectro correspondiente a las frecuencias.
#| echo: false
#| include: true

summary_x %>% ggplot(aes(x=frecuencias, y=varianza)) +
  geom_point(color="#2b8cbe") +
  labs(title="Varianzas por frecuencia", x ="Frecuencia", y = "Varianzas") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
#| label: fig-covs
#| fig-cap: Covarianza de la energía del espectro correspondiente a las frecuencias y el Compuesto $SO_3$.
#| echo: false
#| include: true

df_covs <- as.data.frame(t(as.data.frame(cov(df[,1], df[,-1]))))
names(df_covs) <-  "Covarianza"
df_covs$Variable <- seq(1, nrow(df_covs))

df_covs %>% ggplot(aes(x=Variable, y=Covarianza)) +
  geom_point(color="#2b8cbe") +
  labs(title="Covarianzas por frecuencia", x ="Frecuencia", y = "Covarianzas") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

Para entender cómo se distribuye la que será nuestra variable dependiente podemos utilizar la @fig-hist. En la misma, mediante un histograma de densidad del compuesto $SO_3$, se puede ver que pareciera haber una leve asimetría hacia la derecha y que se trata de una distribución unimodal.

```{r}
#| label: fig-hist
#| fig-cap: Histograma y estimación de densidad del compuesto $SO_3$.
#| echo: false
#| include: true

df %>%
  ggplot(aes(x = comp_6)) +
  geom_histogram(aes(y = after_stat(..density..)),
                 fill = "#2b8cbe", alpha = 0.5, bins = 10) +
  geom_density(color = "#2b8cbe", linewidth = 1.5) +
  theme_minimal() +
  labs(x = expression(SO[3]), y = "Densidad", title = expression("Histograma del compuesto " * S0[3])) +
  theme(plot.title = element_text(hjust=0.5))
```

# Métodos {#sec-modelos}

Dado que el objetivo de este estudio es encontrar un modelo que permita predecir el componente $SO_3$ de vasijas de vidrio de un yacimiento arqueológico, se evaluará primero qué tipo de modelo tiene la mejor capacidad predictiva para este problema en particular.

Para ello, primero se divide el set inicial de datos en *train* y *test*, en esta partición inicial el 80% de los datos se asignó a la muestra de entrenamiento y el 20% restante a la muestra de testeo\footnote{Utilizando el paquete \textit{rsample}.}. Luego se evalúa la *performance* predictiva de los modelos ajustados usando las siguientes metodologías: Regresión Regularizada (*Ridge*, *LASSO* y *Elastic Net*), *Principal Component Regression* (PCR) y *Partial Least Square Regression* (PLS), Árbol de regresión, *Random Forest*, *Boosting* y K-vecinos más cercanos exclusivamente con la muestra de entrenamiento, para luego reportar el error final en la muestra de testeo con el modelo seleccionado.

```{r}
#| label: train-set-split

set.seed(seed)
split <- initial_split(df, prop = 0.8)
train <- training(split)
test <- testing(split)
```

Cabe mencionar que los datos provienen de una espectrometría y cada una de las `r nrow(df)` observaciones es compuesta por `r ncol(df)-1` covariables, representando cada una de ellas la medición de una frecuencia. Por lo tanto, este problema ya parte del hecho de que el número de covariables a usar es mayor al número de observaciones y, por lo tanto, es de esperar que los modelos con mejor capacidad predictiva sean los que abordan de una u otra manera la reducción del número de covariables. Podemos dividir estas metodologías en dos grupos: en primer lugar se encuentran las que incluyen la regularización de las covariables, como es el caso de las regresiones *Ridge*, *LASSO* y *Elastic Net* y en segundo se encuentran PCR y PLS que que reducen la cantidad de covariables en un menor número de componentes. De todas formas en este trabajo se evalúan otros modelos que no cuentan con esta propiedad.

También hay que notar que las técnicas mencionadas incluyen hiper-parámetros. Por lo tanto, se utiliza la validación cruzada para obtener el mejor modelo, que será el que minimice el error cuadrático medio en la muestra de entrenamiento. La validación cruzada consta de dividir aleatoriamente la muestra de entrenamiento en K sub-muestras de aproximadamente igual tamaño y hacer K ajustes. En cada uno de los ajustes se usan los datos pertenecientes a las K-1 sub-muestras y la sub-muestra de validación se utiliza para estimar el error de dicho modelo ajustado. Como este procedimiento se realiza K veces, el error estimado es el promedio de las K estimaciones del error de validación\footnote{Se distingue el error de validación del error de testeo, mientras que el primero es una submuestra de la partición de entrenamiento, el segundo surge una división de los datos orginales.}. Se decidió usar un K=5 para que la partición de validación cruzada no quede demasiado pequeña, entendiendo que contamos únicamente `r nrow(train)` datos en la muestra de entrenamiento. Por último, se comparan los modelos ajustados con la mejor combinación de sus respectivos hiper-parámetros utilizando el error estimado por validación cruzada.

## Reducción de la dimensión: PCR y PLS

Estos métodos tienen como estrategia primero calcular las componentes principales\footnote{La técnica de Componentes Principales es utilizada para reducir la dimensión de los datos de forma de preservar la mayor proporción de variabilidad de los mismos.} de las covariables y luego quedarse con un número menor de estas para usarse como predictoras en el modelo de regresión.

### PCR

Para el caso de PCR, la construcción de las componentes principales se hace buscando explicar la mayor proporción de la variabilidad entre las covariables, por lo que no toma en cuenta la variabilidad que existe entre las covariables y la variable de respuesta. Esto último puede generar que no se obtengan los mejores resultados para el problema de predicción que se busca resolver.

```{r}
#| label: pcr-fit

set.seed(seed)
pcr_fit <- pcr(formula = comp_6 ~ ., data = train, scale. = TRUE,
                  validation = "CV")

pcr_fit_msep <- MSEP(pcr_fit, estimate="CV")
pcr_fit_componentes <- which.min(pcr_fit_msep$val)
pcr_fit_mse <- min(pcr_fit_msep$val)
pca_df <- data.frame(Componentes = seq(1,129),
  pca_mse = t(as.data.frame(pcr_fit_msep$val))[,1])
```

En este trabajo, se implementó PCR\footnote{Mediante el paquete \textit{pls}.} y utilizando validación cruzada se estimó del error de predicción. Puede verse en la @fig-pca que el mejor modelo se construye con `r pcr_fit_componentes` componentes principales como predictoras.

```{r}
#| label: fig-pca
#| fig-cap: MSE en funcón del número de componentes en PCR, estimado por validación cruzada.
#| echo: false
#| include: true

pca_df %>% ggplot(aes(x=Componentes, y=pca_mse)) +
  geom_line(color="#2b8cbe") +
  geom_vline(xintercept = pcr_fit_componentes, linetype="dashed") +
  annotate("text", x=117, y=0.005, label=pcr_fit_componentes, angle=90, size=4) +
  labs(title="PCR - MSE vs Número de componentes", y = "MSE") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))
```

### PLS

Como se vio anteriormente la metodología de PCR no tiene en cuenta la variabilidad que cada covariable tiene con respecto a la variable a predecir en la construcción de las componentes principales, por lo que también se utilizó la metodología de PLS que construye las componentes principales de una forma en que sí intenta maximizar la proporción de variabilidad de la variable a predecir.

```{r}
#| label: pls-fit

set.seed(seed)
pls_fit <- plsr(formula = comp_6 ~ ., data = train, scale. = TRUE, validation = "CV")
pls_fit_msep <- MSEP(pls_fit, estimate = "CV")

pls_fit_componentes <- which.min(pls_fit_msep$val)
pls_fit_mse <- min(pls_fit_msep$val)
pls_df <- data.frame(Componentes = seq(1,129),
  pls_mse = t(as.data.frame(pls_fit_msep$val))[,1])
```

Luego de ajustar el modelo\footnote{Utilizando el paquete \textit{pls}}, se observa en la @fig-pls que el mejor modelo usando esta metodología (obtenido a través de validación cruzada) se logra con `r pls_fit_componentes` componentes principales como predictoras.

```{r}
#| label: fig-pls
#| fig-cap: MSE en funcón del número de componentes en PLS, estimado por validación cruzada.
#| echo: false
#| include: true

pls_df %>% filter(pls_mse<0.02) %>% ggplot(aes(x=Componentes, y=pls_mse)) +
  geom_line(color="#2b8cbe") +
  geom_vline(xintercept = pls_fit_componentes, linetype = "dashed") +
  annotate("text", x=17.5, y=0.005, label=pls_fit_componentes, angle=90, size=4) +
  labs(title="PLS - MSE vs Número de componentes", y = "MSE") +
  theme_minimal() +
  theme(plot.title = element_text(hjust = 0.5))

```

## Regularización: Ridge, LASSO y Elastic Net

Las técnicas de regularización empleadas en esta sección\footnote{Cuya implementación se realizó utilizando el paquete \textit{caret}} agregan una penalización al tamaño de los coeficientes en la estimación de los parámetros de la regresión lineal. Cabe mencionar que esto último introduce hiper-parámetros que serán elegidos por validación cruzada con el objetivo de obtener la combinación de los mismos que minimice el error de predicción.

```{r}
#| label: reg-fit

alphas <- seq(0,1, 0.1)
lambdas <- c()
mse <- c()
for(i in alphas){
  set.seed(seed)
  fitted_glmnet <- cv.glmnet(as.matrix(train[,-1]), as.matrix(train[,1]), nfolds=5, type.measure="mse", alpha=i)
  lambdas <- append(lambdas, fitted_glmnet$lambda.min)
  mse <- append(mse, min(fitted_glmnet$cvm))
}

results_elastic_net <- data.frame(Alpha = alphas, Lambda= lambdas, MSE= mse) %>%
  arrange(MSE) %>%
  mutate(log_lambda = log(Lambda))
```

### Ridge

La regularización *Ridge* tiene como principal característica que $\lambda$, el hiper-parámetro que regula cuán estricta tiene que ser la penalización incluida, comprime los coeficientes pero no permite que ninguno llegue a ser 0. Se puede ver en la @fig-ridge-cv que el $log(\lambda)$ que minimiza el error de predicción es `r results_elastic_net %>% filter(Alpha==0) %>% select(log_lambda) %>% first() %>% pull() %>% round(3)`.

```{r}
#| label: ridge-fit

set.seed(seed)
ridge_model <- cv.glmnet(as.matrix(train[,-1]),as.matrix(train[,1]), nfolds=5,type.measure="mse", alpha=0)
ridge_df <- data.frame(lambda_log=log(ridge_model$lambda),
                       mse = ridge_model$cvm)
```

```{r}
#| label: fig-ridge-cv
#| fig-cap: Estimación del error cuadrático medio por validación cruzada en la muestra de entrenamiento para distintos valores de $\log(\lambda)$ en el modelo Ridge.
#| echo: false
#| include: true

plot(ridge_model)
```

### LASSO

A diferencia de *Ridge*, en *LASSO* los coeficientes sí pueden truncarse hasta el 0, por lo que el término de *shrinkage* del modelo anterior puede considerarse más suave. Para nuestra aplicación podemos ver en la @fig-lasso-cv que el $log(\lambda)$ que minimiza el MSE es `r results_elastic_net %>% filter(Alpha==1) %>% select(log_lambda) %>% first() %>% pull() %>% round(3)`.

```{r}
#| label: lasso-fit

set.seed(seed)
lasso_model <- cv.glmnet(as.matrix(train[,-1]),as.matrix(train[,1]), nfolds=5,type.measure="mse", alpha=1)

lasso_df <- data.frame(lambda_log = log(lasso_model$lambda),
                       mse = lasso_model$cvm)
```

```{r}
#| label: fig-lasso-cv
#| fig-cap: Estimación del error cuadrático medio por validación cruzada en la muestra de entrenamiento para distintos valores de $\log(\lambda)$ en el modelo LASSO.
#| echo: false
#| include: true

plot(lasso_model, col="blue")
```

### Elastic Net

El modelo de \textit{Elastic Net} es una combinación lineal de las penalizaciones utilizadas por \textit{Ridge} y \textit{LASSO}, por lo que se le suma un hiper-parámetro adicional ($\alpha$) que pondera cuanto más pesa una penalización por sobre la otra. Así, para \textit{Elastic Net} también se utilizó la técnica de validación cruzada para hallar el mejor modelo, ahora eligiendo la combinación de hiper-parámetros $\alpha$ y $\lambda$ que generen el menor error de predicción en la muestra de entrenamiento.

En la @fig-elastic-cv se puede ver que el $log(\lambda)$ que minimiza el error de predicción es `r results_elastic_net %>% select(log_lambda) %>% first() %>% pull() %>% round(3)` para un $\alpha$ de `r results_elastic_net %>% select(Alpha) %>% first() %>% pull() %>% round(3)`.

```{r}
#| label: elastic-net-final-fit

set.seed(seed)
elastic_model <- cv.glmnet(as.matrix(train[,-1]),as.matrix(train[,1]), nfolds=5,type.measure="mse", alpha=0.9)

elastic_df <- data.frame(lambda_log = log(elastic_model$lambda),
                       mse = elastic_model$cvm)
```

```{r}
#| label: fig-elastic-cv
#| fig-cap: Estimación del error cuadrático medio por validación cruzada en la muestra de entrenamiento para distintos valores de $\log(\lambda)$ en el modelo Elastic net con un $\alpha=0.9$.
#| echo: false
#| include: true

plot(elastic_model)
```

## Árbol de regresión

El método del árbol de regresión consiste en hacer particiones recursivas en el espacio de covariables de forma tal de obtener subconjuntos terminales disjuntos (las hojas o nodos terminales del árbol) lo más homogéneos posibles. Por lo tanto, el árbol se construye para definir regiones en las cuales las observaciones que caigan en cada región sean lo más parecidas entre ellas.

En este trabajo se decidió hacer una implementación de este modelo\footnote{Mediante el paquete \textit{caret}} y se buscó encontrar el árbol con menor error de predicción por medio de validación cruzada, comparando entre árboles con distinto número de nodos terminales. Siendo este último el hiper-parámetro para el cual exploraremos valores que van desde el 1 al 15.

```{r}
#| label: tree-fit

set.seed(seed)
tree_fit <- train(comp_6 ~ ., train, 
                   method = "rpart2", 
                   tuneLength = 10,
                   tuneGrid = expand.grid(maxdepth = seq(1,15)),
    trControl = trainControl(method = "cv", number = 5))

tree_fit_df <- tree_fit$results
tree_fit_df$mse <- tree_fit_df$RMSE^2
```

En la @fig-tree-plot vemos que el árbol con menor MSE es el que tiene 3 nodos terminales\footnote{Este resultado fue muy sensible a distintas semillas, pero las estimaciones del MSE se mantuvieron en rangos similares, siempre por encima del resto de los modelos.}, por lo qué podemos referirnos a la @fig-prunned-tree para ver que solamente se usaron las covariables V243 y V122 para para predecir. Si bien este modelo es simple e interpretable, su capacidad predictiva no fue muy alta en comparación con los otros modelos evaluados, como se detallará en la próxima sección. Por lo que a continuación se utilizarán dos métodos de ensambles que se espera mejoren la *performance*.

```{r}
#| label: fig-tree-plot
#| fig-cap: MSE de arboles con distintos nodos terminales.
#| echo: false
#| include: true

tree_fit_df %>% 
  ggplot(aes(x = maxdepth, y = mse)) +
  geom_point(color="#2b8cbe") +
  geom_path(color="#2b8cbe") +  
  theme_minimal() +
  labs(x = "Complejidad del árbol", y = "MSE", title = "Resultados Arbol de regresión") +
  scale_x_continuous(breaks = 1:15) +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
#| label: fig-prunned-tree
#| fig-cap: Arbol de regresión podado a 3 nodos terminales.
#| echo: false
#| include: true

tree <- rpart(comp_6 ~ ., data=train, control=rpart.control(cp=0))
best <- tree$cptable[which.min(tree$cptable[,"xerror"]),"CP"]

pruned_tree <- prune(tree, cp=0.15)

prp(pruned_tree, main="Árbol con menor error de predicción")
```

## Ensambles de árboles

Los ensambles se basan en combinar muchos árboles (generalmente sencillos) para obtener un único modelo. Esto se busca porque, pese a que los árboles de decisión tienen la ventaja de ser fácilmente interpretables, generalmente su poder de predicción es bajo. Por este motivo se implementó *Random Forest* y *Boosting*. \footnote{Para los dos modelos se utilizó el paquete \textit{caret}}.

### Random Forest

*Random Forest* construye una cierta cantidad de árboles y reporta el promedio de las predicciones. Para crear los árboles usa un sub-conjunto de las covariables, de esta forma se logra que sean menos dependientes entre sí y reducir la varianza. Tanto la cantidad de árboles a generar como el subconjunto de covariables son considerados hiper-parámetros en este modelo, por lo que se decidió probar distintas combinaciones de estos para encontrar la que tenga menor MSE por validación cruzada. En relación con la cantidad de árboles, se exploraron valores en el rango de 50 a 500, con incrementos de 50. Mientras que en cuanto a los subconjuntos de covariables, se optó por evaluar modelos que utilicen 2, 6 y 10 covariables para crear los árboles.

```{r}
#| label: rf-fit

modellist <- list()
tree_sec <- seq(50, 500, 50)


for (ntree in tree_sec){
  set.seed(seed)
  fit <- train(comp_6~.,
               data = train,
               method = 'rf',
               tuneGrid = expand.grid(mtry = c(2,6,10)),
               trControl = trainControl(method  = "cv", number  = 5),
               ntree = ntree)
  key <- toString(ntree)
  modellist[[key]] <- fit}

results_rf <- do.call(rbind, Map(function(x) modellist[[x]]$results, as.character(tree_sec)))
results_rf$n_trees <- sort(rep(tree_sec, length(unique(results_rf$mtry))))
results_rf$mse <- results_rf$RMSE^2
results_rf %>% arrange(mse)
```

Como se puede ver en la @fig-rf-plot los mejores resultados fueron obtenidos con `r results_rf %>% arrange(mse) %>% select(n_trees) %>% first() %>% pull()` arboles para un sub-conjunto de `r results_rf %>% arrange(mse) %>% select(mtry) %>% first() %>% pull()` covariables. También podemos estudiar en la @fig-importance-rf que las covariables más importantes para predecir el $SO_3$ fueron V246 y V248.

```{r}
#| label: fig-rf-plot
#| fig-cap: MSE del modelo Random Forest para distintas combinaciones de hiper-parámetros. Destacamos que mtry refiere a la cantidad de covariables utilizadas.
#| echo: false
#| include: true

results_rf %>% 
  ggplot(aes(x = n_trees, y = mse, color = as.factor(mtry))) +
  geom_point() +
  geom_path(aes(group = as.factor(mtry))) +  
  scale_color_brewer(palette = "Dark2") +
  theme_minimal() +
  labs(x = "Número de árboles", y = "MSE", title = "Resultados Random Forest", color='mtry') +
  theme(plot.title = element_text(hjust = 0.5))

```

```{r}
#| label: importance-rf

set.seed(seed)
rf_fit <- train(comp_6~.,
               data = train,
               method = 'rf',
               tuneGrid = expand.grid(mtry = c(6)),
               trControl = trainControl(method  = "cv", number  = 5),
               ntree = 250)

rf_imp <- data.frame(
  variables = names(df)[-1],
  importancias=rf_fit$finalModel$importance)
```

```{r}
#| label: fig-importance-rf
#| fig-cap: 10 covariables más importantes en el modelo Random Forest ordenadas de mayor a menor.
#| echo: false
#| include: true

rf_imp %>%arrange(-IncNodePurity)  %>%head(10) %>% ggplot(aes(x = reorder(variables, IncNodePurity), y = IncNodePurity)) +
  geom_bar(stat = "identity", fill = "#2b8cbe") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Importancias Random Forest", x = "Covariable", y = "Importancia") +
  theme(plot.title = element_text(hjust = 0.5))
```

### Boosting

En cambio en *Boosting* los árboles se crean secuencialmente utilizando información sobre los anteriores. Es decir que cada árbol se crea sobre los residuos del árbol anterior. De esta forma se pueden mejorar las predicciones de forma paulatina y así evitar el *overfitting*. En este estudio los hiper-parámetros evaluados fueron la cantidad de árboles y el *shrinkage* ó *learninig rate* que se usa para regular cuanto se utiliza la información de los árboles anteriores. La búsqueda de los hiper-parámetros se realizó para valores de árboles que van desde 100 hasta 2000 en incrementos de a 200 para los siguientes valores de *shrinkage*: 0.001, 0.01 y 0.01.

```{r}
#| label: gbm-fit

set.seed(seed)

hyperparameter_gbm <- expand.grid(
  n.trees = seq(100, 2000, 200),
  interaction.depth = 5,
  shrinkage = c(0.001, 0.01, 0.1),
  n.minobsinnode = 5)

gbm_fit <- train(
  comp_6 ~ ., 
  data = train, 
  method = "gbm",
  trControl = trainControl(method  = "cv", number  = 5),
  tuneGrid = hyperparameter_gbm,
  verbose = FALSE)

metrics_gbm_df <- gbm_fit$results
metrics_gbm_df$mse <- metrics_gbm_df$RMSE^2
metrics_gbm_df %>% arrange(mse)
```

Al calcular el MSE para las distintas combinaciones de hiper-parámetros podemos ver en la @fig-gbm-plot que el mínimo se alcanza en `r metrics_gbm_df %>% arrange(mse) %>% select(n.trees) %>% first() %>% pull()` árboles con un *shrinkage* de `r metrics_gbm_df %>% arrange(mse) %>% select(shrinkage) %>% first() %>% pull()`. Para entender un poco más qué covariables utilizó el modelo con menor error de predicción podemos utilizar la @fig-importance-gbm donde vemos que V120 fue la covariable más importante en predecir el compuesto $SO_3$.

```{r}
#| label: fig-gbm-plot
#| fig-cap: MSE del modelo Boosting para distintas combinaciones de hiper-parámetros.
#| echo: false
#| include: true

metrics_gbm_df %>% 
  ggplot(aes(x = n.trees, y = mse, color = as.factor(shrinkage))) +
  geom_point() +
  geom_path(aes(group = as.factor(shrinkage))) +  
  scale_color_brewer(palette = "Dark2") +
  theme_minimal() +
  labs(x = "Número de árboles", y = "MSE", title = "Resultados Boosting", color='Shrinkage') +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
#| label: importance-gbm

imp_gbm <- varImp(gbm_fit$finalModel, scale = FALSE)
imp_gbm$rel_imp_perc <- imp_gbm$Overall/sum(imp_gbm$Overall) ; imp_gbm$var <- names(df)[-1]
```

```{r}
#| label: fig-importance-gbm
#| fig-cap: 10 covariables más importantes en el modelo Boosting ordenadas de mayor a menor.
#| echo: false
#| include: true

imp_gbm %>% arrange(-Overall)%>%head(10) %>% ggplot(aes(x = reorder(var, Overall), y = Overall)) +
  geom_bar(stat = "identity", fill = "#2b8cbe") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Importancias Boosting", x = "Covariable", y = "Importancia") +
  theme(plot.title = element_text(hjust = 0.5))

```

## K vecinos cercanos

Este modelo se basa en promediar los valores de los k vecinos más cercanos a $x$ en el espacio definido por las covariables. El hiper-parámetro con el que se cuenta es $k$ que refiere al número de vecinos que se tendrá en cuenta para calcular el promedio. En este trabajo se evaluaron valores de k que van desde 1 hasta 10\footnote{La implementación se realizó utilizando el paquete \textit{caret}.}.

```{r}
#| label: knn-fit

train_scaled <- as.data.frame(train %>% select(-comp_6) %>% scale())
train_scaled$comp_6 <- train$comp_6

set.seed(seed)
knn_fit <- train(comp_6 ~ .,
             method     = "knn",
             tuneGrid   = expand.grid(k = 1:10),
             trControl  = trainControl(method  = "cv", number  = 5),
             data       = train_scaled)

metrics_knn_df <- knn_fit$results
metrics_knn_df$mse <- metrics_knn_df$RMSE^2
metrics_knn_df %>% arrange(mse)
```

Se puede ver en la @fig-knn-plot que el número de vecinos que minimiza el MSE es `r metrics_knn_df %>% arrange(mse) %>% select(k) %>% first() %>% pull()`. Cabe destacar que este modelo es sensible a cambios de escala, por lo que estandarizamos las covariables restando la media y dividiendo por el desvío para obtener mejores resultados.

```{r}
#| label: fig-knn-plot
#| fig-cap: Error cuadrático medio del modelo KNN para distintos valores de k.
#| echo: false
#| include: true

metrics_knn_df %>% arrange(k)%>% ggplot(aes(x=k, y=mse)) +
  geom_point(color="#2b8cbe") +
  geom_path(color="#2b8cbe") + 
  theme_minimal() +
  labs(y="MSE", title="Resultados K-vecinos") +
  scale_x_continuous(breaks = 1:10) +
  theme(plot.title = element_text(hjust = 0.5))
```

# Comparación de resultados {#sec-comp-resultados}

Con lo expuesto previamente se ajustaron cada uno de los modelos mencionados de forma de obtener el "mejor" de cada uno de ellos -es decir, el que tenga la combinación de hiper-parámetros que minimice el menor error de predicción estimado por validación cruzada en la muestra de entrenamiento. Así, se obtuvieron los siguientes errores con cada modelo, mostrados en la @tbl-resultados-modelos.

```{r}
#| label: results-table-create

resultados_modelos <- list(
  PCR = pcr_fit_mse,
  PLS = pls_fit_mse,
  Ridge = results_elastic_net %>% filter(Alpha == 0) %>% select(MSE) %>% pull(),
  LASSO = results_elastic_net %>% filter(Alpha == 1) %>% select(MSE) %>% pull(),
  "Elastic Net" = results_elastic_net %>% arrange(MSE) %>% select(MSE) %>% first() %>% pull(),
  'Random Forest' = results_rf %>% arrange(mse) %>% select(mse) %>% first() %>% pull(),
  Boosting = metrics_gbm_df %>% arrange(mse) %>% select(mse) %>% first() %>% pull(),
  "Arbol de Regresión" = tree_fit_df %>% arrange(mse) %>% select(mse) %>% first() %>% pull(),
  'K vecinos'= metrics_knn_df %>% arrange(k) %>% select(mse) %>% first() %>% pull()
  )

df_resultados_modelos <- data.frame(t(data.frame(resultados_modelos)))
df_resultados_modelos$modelo<- row.names(df_resultados_modelos)
names(df_resultados_modelos) <- c("MSE", "Modelo") ; rownames(df_resultados_modelos) <- NULL

df_resultados_modelos <- df_resultados_modelos %>%
  arrange(MSE) %>%
  select(Modelo, `MSE`) %>%
  mutate(MSE=round(MSE,5)) %>%
  mutate(Modelo = gsub(".", " ", Modelo, fixed=TRUE),Modelo = case_when(
    Modelo == "K vecinos" ~ "K-vecinos",
    TRUE ~ Modelo ),
    MSE = format(MSE, scientific = TRUE))
```

```{r}
#| label: tbl-resultados-modelos
#| tbl-cap: Comparación de modelos usando las estimaciones del error de predicción con validación cruzada en los datos de entrenamiento.
#| echo: false
#| include: true

kable(df_resultados_modelos)
```

Se puede ver que el modelo con mejor performance predictiva fue *Elastic Net*, seguido muy de cerca por *LASSO*. También observamos que casi todos los modelos que de alguna forma reducen el número de covariables\footnote{Los modelos son \textit{Elastic Net}, \textit{LASSO}, \textit{Ridge}, PCR y PLS} tienen una mejor *performance* que los demás (a excepción de *Ridge*), esto era de esperarse dado que los datos con lo que contamos tienen más covariables que observaciones. Finalmente, notamos que tanto *Boosting* como *Random Forest* tuvieron un mejor desempeño, en términos del error cuadrático medio, que el Árbol de regresión. Esto último también podía preverse dado que los ensambles de árboles se crearon justamente para mejorar el poder predictivo de los árboles individuales.

# Supuestos {#sec-supuestos}

```{r}
#| label: coefs-elastic

coef_elastic <- as.data.frame(coef(elastic_model)[,1])
names(coef_elastic) <- c("Coefs")
```

Una vez elegido el modelo con mejor capacidad predictiva en nuestros datos de entrenamiento, el cual es *Elastic Net*, se procede a estudiarlo con mayor profundidad. De forma contra intuitiva se puede ver en la  @fig-coef-glmnet que los coeficientes distintos de cero (y más grandes en valor absoluto), los cuales son `r sum(coef_elastic$Coefs!=0)`, no coinciden con las frecuencias que se habían comentado en la sección exploratoria que eran las más "informativas", en el sentido que tenían mayor varianza en sí mismas y covarianza con la variable de respuesta. Sin embargo, no se encontró una razón "intuitiva" de este hecho, y se lo atribuye a la heurísitca de cómo el procedimiento de *Elastic Net* aplicando las penalizaciones L1 y L2.

```{r}
#| label: fig-coef-glmnet
#| fig-cap: Coeficientes del modelo ajustado Elastic Net para cada frecuencia.
#| echo: false
#| include: true

coef_elastic %>% rownames_to_column() %>% 
  rename("Estimate"=Coefs) %>%
  mutate(Coeficientes = seq(1, nrow(coef_elastic))) %>%
  filter(Coeficientes !=1) %>%
  ggplot(aes(x=Coeficientes, y=Estimate))+ geom_line(color="#2b8cbe") +
  labs(title="Coeficientes Elastic Net") +
  theme_minimal() + 
  theme(plot.title = element_text(hjust = 0.5))
```

Por otro lado, para corroborar los supuestos de regresión lineal se realizaron los siguientes dos gráficos: en el primero se ven los residuos versus los valores ajustados (@fig-res-ajus), y en el segundo un QQ-plot de los residuos (@fig-qqplot). Como se puede notar de los mismos no se percibe una estructura en los residuos que pueda indicar un problema con el modelo, y además los residuos parecen cumplir el supuesto de normalidad (no se ven colas pesadas que nos alertasen de posibles *outliers*). Por lo tanto, concluimos que por el análisis gráfico, pareciera que el modelo propuesto cumple los supuestos de regresión lineal.

```{r}
test_res <- test %>% mutate(
  preds_mod_4 = predict(elastic_model, as.matrix(test[,-1])),
  res_mod_4 = comp_6 - preds_mod_4)
```

```{r}
#| label: fig-res-ajus
#| fig-cap: Valores ajustados versus resudiuos en muestra de validación.
#| echo: false
#| include: true

test_res %>% ggplot(aes(x=preds_mod_4, y=res_mod_4)) +
  geom_point(color="#2b8cbe") +
  theme_minimal() +
  labs(title="Residuos vs Predicciones",
       x ="Predicciones", y = "Residuos") +
  theme(plot.title = element_text(hjust = 0.5))
```

```{r}
#| label: fig-qqplot
#| fig-cap: QQ-plot de los residuos del modelo Elastic Net.
#| echo: false
#| include: true

test_res %>% ggplot(aes(sample=res_mod_4)) +
  stat_qq(color="#2b8cbe") + 
  stat_qq_line() +
  theme_minimal() +
  labs(title="QQ-Plot de Residuos",
       x ="Cuantiles teóricos", y = "Cuantiles muestrales")+
  theme(plot.title = element_text(hjust = 0.5))
```

# Conclusión {#sec-conclusiones}

El modelo que mejor capacidad predictiva tiene para nuestro objetivo de predecir el componente $SO_3$ presente en las vasijas, y sujeto a los datos con los que contamos, es el modelo ajustado *Elastic Net*. El error de predicción (MSE) estimado en la muestra de testeo es `r round(sqrt(mean((test_res$preds_mod_4-test_res$comp_6)^2)), 3)`. Cabe mencionar que esta estimación final del error fue hecha con datos que el modelo no utilizó para su ajuste ni para la comparación entre los otros modelos probados, ya que esas fueron hechas con los datos de entrenamiento y la técnica de validación cruzada para las estimaciones de los errores de cada modelo en dichos datos.

{{< pagebreak >}}

# Apéndice {.unnumbered}

```{r}
#| label: tbl-results-pcr
#| tbl-cap: Resulados de los 10 modelos PCR con menor error cuadrático medio.
#| echo: false
#| include: true

kable(pca_df %>%
        rownames_to_column() %>%
        select(Componentes, pca_mse) %>%
        arrange(pca_mse) %>%
        rename("MSE"=pca_mse) %>%
        mutate(MSE = format(round(MSE, 5), scientific = TRUE)) %>%
        head(10),
      align=rep('c', 2))
```

```{r}
#| label: tbl-results-pls
#| tbl-cap: Resulados de los 10 modelos PLS con menor error cuadrático medio.
#| echo: false
#| include: true

kable(pls_df %>%
        rownames_to_column() %>%
        select(Componentes, pls_mse) %>%
        arrange(pls_mse) %>%
        rename("MSE" = pls_mse) %>%
        head(10) %>%
        mutate(MSE = format(round(MSE, 5), scientific = TRUE))
        ,
      align=rep('c', 2))
```

```{r}
#| label: tbl-results-reg
#| tbl-cap: Resultados para valores de $\alpha$ que van desde 0 hasta 1 en Elastic-net junto con el $\lambda$ óptimo obtenido por validación cruzada. Se incluyendo Ridge ($\alpha=0$) y LASSO ($\alpha=1$).
#| echo: false
#| include: true

kable(results_elastic_net %>%
  select(Alpha, log_lambda, MSE) %>%
  mutate(MSE = format(round(MSE, 5), scientific = TRUE),
         log_lambda = round(log_lambda, 3)) %>%
  rename("Log Lambda" = log_lambda)
  ,
  align=rep('c', 3))
```

```{r}
#| label: tbl-results-tree
#| tbl-cap: Error cuadrático medio de árboles de predicción con nodos terminales que van desde el 1 hasta el 15.
#| echo: false
#| include: true

kable(tree_fit_df %>%
        arrange(mse) %>%
        rownames_to_column() %>%
        select(-rowname) %>%
        select(maxdepth, mse) %>%
        rename("Nodos terminales" = maxdepth, "MSE" = mse) %>%
        mutate(MSE = format(round(MSE,4), scientific = TRUE)),
      align=rep('c', 2)) 

```

```{r}
#| label: tbl-results-rf
#| tbl-cap: Error cuadrático medio para las combinaciones de hiperpárametros testeadas en Random Forest.
#| echo: false
#| include: true

kable(results_rf %>%
        arrange(mse) %>%
        rownames_to_column() %>%
        select(-rowname) %>%
        select(n_trees, mtry, mse) %>%
        mutate(mse=round(mse,5)) %>%
        rename("Número de árboles"=n_trees, "MSE"=mse) %>%
        mutate(MSE = format(round(MSE,5), scientific = TRUE)) %>%
        head(10),
      align=rep('c', 3)) 
```

```{r}
#| label: tbl-results-gbm
#| tbl-cap: Error cuadrático medio para las combinaciones de hiperpárametros testeadas en Boosting.
#| echo: false
#| include: true

kable(metrics_gbm_df %>%
        arrange(mse) %>%
        rownames_to_column() %>%
        select(-rowname) %>%
        select(n.trees, shrinkage, mse) %>%
        rename("Número de árboles"=n.trees, "MSE"=mse) %>%
        mutate(MSE = format(round(MSE,5), scientific = TRUE)),
      align=rep('c', 3)) 
```

```{r}
#| label: tbl-results-knn
#| tbl-cap: Error cuadrático medio asociado a modelos de K-vecinos cuando k toma valores de 1 hasta el 10.
#| echo: false
#| include: true

kable(metrics_knn_df %>%
        arrange(mse) %>%
        rownames_to_column() %>%
        select(-rowname) %>%
        select(k, mse) %>%
        rename("MSE"=mse) %>%
        mutate(MSE = format(round(MSE,6), scientific = TRUE)),
      align=rep('c', 2))
```

{{< pagebreak >}}

# Referencias {.unnumbered}

James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning: with applications in R. Corrected edition. New York, Springer.

Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). The elements of statistical learning: data mining, inference, and prediction. 2nd ed. New York, Springer.

R Core Team (2020). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. [https://www.R-project.org/](https://www.R-project.org/)



