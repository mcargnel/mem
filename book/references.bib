
# ------------------
# Profe
# ------------------

@article{buhlamn_2012,
  author  = {B{\"u}hlmann, Peter},
  year    = {2012},
  month   = {01},
  pages   = {},
  title   = {Bagging, Boosting and Ensemble Methods},
  isbn    = {978-3-642-21550-6},
  journal = {Handbook of Computational Statistics},
  doi     = {10.1007/978-3-642-21551-3_33}
}

@article{moreira_2012,
  author  = {Moreira, Jo{\~a}o and Soares, Carlos and Jorge, Al{\'i}pio and Sousa, Jorge},
  year    = {2012},
  month   = {11},
  pages   = {10:1-10:40},
  title   = {Ensemble Approaches for Regression: A Survey},
  volume  = {45},
  journal = {ACM Computing Surveys},
  doi     = {10.1145/2379776.2379786}
}

@article{berk_2016,
  author  = {Berk, Richard},
  year    = {2006},
  month   = {02},
  pages   = {},
  title   = {An Introduction to Ensemble Methods for Data Analysis},
  volume  = {34},
  journal = {Sociological Methods & Research},
  doi     = {10.1177/0049124105283119}
}

@article{freedman_81,
  author    = {D. A. Freedman},
  title     = {{Bootstrapping Regression Models}},
  volume    = {9},
  journal   = {The Annals of Statistics},
  number    = {6},
  publisher = {Institute of Mathematical Statistics},
  pages     = {1218 -- 1228},
  keywords  = {bootstrap, Correlation, least squares, regression, Wasserstein metrics},
  year      = {1981},
  doi       = {10.1214/aos/1176345638},
  url       = {https://doi.org/10.1214/aos/1176345638}
}

@article{Breiman1996,
  title   = {Bagging predictors},
  author  = {Breiman, Leo},
  journal = {Machine Learning},
  volume  = {24},
  number  = {2},
  pages   = {123--140},
  year    = {1996},
  doi     = {10.1007/BF00058655}
}


@article{Goldstein2015,
  author    = {Alex Goldstein and Adam Kapelner and Justin Bleich and Emil Pitkin},
  title     = {Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation},
  journal   = {Journal of Computational and Graphical Statistics},
  volume    = {24},
  number    = {1},
  pages     = {44--65},
  year      = {2015},
  publisher = {Taylor \& Francis},
  doi       = {10.1080/10618600.2014.907095},
  url       = { https://doi.org/10.1080/10618600.2014.907095 },
  eprint    = { https://doi.org/10.1080/10618600.2014.907095 }
}

# ------------------
# Libros
# ------------------
@book{CART,
  author    = {Leo Breiman and Jerome H. Friedman and Richard A. Olshen and Charles J. Stone},
  title     = {Classification and Regression Trees},
  year      = {1984},
  publisher = {Routledge},
  doi       = {10.1201/9781315139470}
}

@book{ISLP,
  title     = {An Introduction to Statistical Learning: with Applications in R},
  author    = {Gareth James and Daniela Witten and Trevor Hastie and Robert Tibshirani},
  year      = {2023},
  publisher = {Springer},
  doi       = {10.1007/978-3-031-38747-0}
}

@book{ESL,
  title     = {The Elements of Statistical Learning},
  author    = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
  year      = {2009},
  publisher = {Springer},
  doi       = {10.1007/978-0-387-84858-7}
}

@book{efron1994boostrap,
  title     = {An Introduction to the Bootstrap},
  author    = {Efron, B. and Tibshirani, R.J.},
  isbn      = {9780412042317},
  lccn      = {93004489},
  series    = {Chapman \& Hall/CRC Monographs on Statistics \& Applied Probability},
  url       = {https://books.google.com.ar/books?id=gLlpIUxRntoC},
  year      = {1994},
  publisher = {Taylor \& Francis}
}

# ------------------
# Filosoficos
# ------------------

@article{breiman_2001,
  author    = {Leo Breiman},
  title     = {{Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)}},
  volume    = {16},
  journal   = {Statistical Science},
  number    = {3},
  publisher = {Institute of Mathematical Statistics},
  pages     = {199 -- 231},
  year      = {2001},
  doi       = {10.1214/ss/1009213726},
  url       = {https://doi.org/10.1214/ss/1009213726}
}

@article{to_explain_or_to_predict,
  author    = {Galit Shmueli},
  title     = {{To Explain or to Predict?}},
  volume    = {25},
  journal   = {Statistical Science},
  number    = {3},
  publisher = {Institute of Mathematical Statistics},
  pages     = {289 -- 310},
  keywords  = {causality, data mining, Explanatory modeling, predictive modeling, predictive power, scientific research, statistical strategy},
  year      = {2010},
  doi       = {10.1214/10-STS330},
  url       = {https://doi.org/10.1214/10-STS330}
}

# ------------------
# Other
# ------------------

@article{freiesleben_2024,
  author  = {Freiesleben, Timo and K{\"o}nig, Gunnar and Molnar, Christoph and Tejero-Cantero, {\'A}lvaro},
  year    = {2024},
  month   = {07},
  pages   = {},
  title   = {Scientific Inference with Interpretable Machine Learning: Analyzing Models to Learn About Real-World Phenomena},
  volume  = {34},
  journal = {Minds and Machines},
  doi     = {10.1007/s11023-024-09691-z}
}

@article{friedman_2001,
  author    = {Jerome H. Friedman},
  title     = {{Greedy function approximation: A gradient boosting machine.}},
  volume    = {29},
  journal   = {The Annals of Statistics},
  number    = {5},
  publisher = {Institute of Mathematical Statistics},
  pages     = {1189 -- 1232},
  keywords  = {boosting, decision trees, Function estimation, robust nonparametric regression},
  year      = {2001},
  doi       = {10.1214/aos/1013203451},
  url       = {https://doi.org/10.1214/aos/1013203451}
}

@article{breiman2001a,
  author   = {Leo Breiman},
  title    = {Random Forests},
  journal  = {Machine Learning},
  year     = {2001},
  volume   = {45},
  number   = {1},
  pages    = {5--32},
  doi      = {10.1023/A:1010933404324},
  url      = {https://doi.org/10.1023/A:1010933404324},
  abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148â€“156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
  issn     = {1573-0565}
}

@article{strobl_2008,
  author  = {Strobl, Carolin and Boulesteix, Anne-Laure and Kneib, Thomas and Augustin, Thomas and Zeileis, Achim},
  year    = {2008},
  month   = {08},
  pages   = {307},
  title   = {Conditional Variable Importance for Random Forests},
  volume  = {9},
  journal = {BMC bioinformatics},
  doi     = {10.1186/1471-2105-9-307}
}

@article{fischer_2019,
  author  = {Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca},
  year    = {2019},
  month   = {01},
  pages   = {},
  title   = {All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously},
  volume  = {20},
  journal = {Journal of machine learning research : JMLR}
}
@article{natekin_knoll_2013,
  author   = {Natekin, Alexey  and Knoll, Alois },
  title    = {Gradient boosting machines, a tutorial},
  journal  = {Frontiers in Neurorobotics},
  volume   = {7},
  year     = {2013},
  url      = {https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2013.00021},
  doi      = {10.3389/fnbot.2013.00021},
  issn     = {1662-5218},
  abstract = {<p>Gradient boosting machines are a family of powerful machine-learning techniques that have shown considerable success in a wide range of practical applications. They are highly customizable to the particular needs of the application, like being learned with respect to different loss functions. This article gives a tutorial introduction into the methodology of gradient boosting methods with a strong focus on machine learning aspects of modeling. A theoretical information is complemented with descriptive examples and illustrations which cover all the stages of the gradient boosting model design. Considerations on handling the model complexity are discussed. Thr
              ee practical examples of gradient boosting applications are presented and comprehensively analyzed.</p>}
}

@inproceedings{pearl_2018,
  author    = {Pearl, Judea},
  title     = {Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution},
  year      = {2018},
  isbn      = {9781450355810},
  publisher = {Association for Computing Machinery},
  address   = {New York, NY, USA},
  url       = {https://doi.org/10.1145/3159652.3176182},
  doi       = {10.1145/3159652.3176182},
  abstract  = {Current machine learning systems operate, almost exclusively, in a statistical, or model-blind mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal inference.},
  booktitle = {Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
  pages     = {3},
  numpages  = {1},
  keywords  = {keynote talk},
  location  = {Marina Del Rey, CA, USA},
  series    = {WSDM '18}
}

@article{Zhao_2021,
  author    = {Qingyuan Zhao and Trevor Hastie},
  title     = {Causal Interpretations of Black-Box Models},
  journal   = {Journal of Business \& Economic Statistics},
  volume    = {39},
  number    = {1},
  pages     = {272--281},
  year      = {2021},
  publisher = {Taylor \& Francis},
  doi       = {10.1080/07350015.2019.1624293},
  url       = {https://doi.org/10.1080/07350015.2019.1624293}
}

@article{Pearl1993,
  author  = {Pearl, Judea},
  title   = {Comment: Graphical Models, Causality and Intervention},
  journal = {Statistical Science},
  volume  = {8},
  number  = {3},
  pages   = {266--269},
  year    = {1993},
  doi     = {10.1214/ss/1177010894},
  url     = {https://doi.org/10.1214/ss/1177010894}
}

# ------------------
# Datos
# ------------------

@misc{diabetes_34,
  author       = {Kahn, Michael},
  title        = {{Diabetes}},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5T59G}
}

@misc{abalone_1,
  author       = {Nash, Warwick and Sellers, Tracy and Talbot, Simon and Cawthorn, Andrew and Ford, Wes},
  title        = {{Abalone}},
  year         = {1994},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C55C7W}
}

@misc{airfoil_self-noise_291,
  author       = {Brooks, Thomas and Pope, D. and Marcolini, Michael},
  title        = {{Airfoil Self-Noise}},
  year         = {1989},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5VW2C}
}

@misc{concrete_compressive_strength_165,
  author       = {Yeh, I-Cheng},
  title        = {{Concrete Compressive Strength}},
  year         = {1998},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5PK67}
}

@misc{wine_109,
  author       = {Aeberhard, Stefan and Forina, M.},
  title        = {{Wine}},
  year         = {1992},
  howpublished = {UCI Machine Learning Repository},
  note         = {{DOI}: https://doi.org/10.24432/C5PC7J}
}

# ------------------
# Arxiv / Not used
# ------------------

@misc{mentch_2021,
  title         = {Bridging Breiman's Brook: From Algorithmic Modeling to Statistical Learning},
  author        = {Lucas Mentch and Giles Hooker},
  year          = {2021},
  eprint        = {2102.12328},
  archiveprefix = {arXiv},
  primaryclass  = {stat.OT},
  url           = {https://arxiv.org/abs/2102.12328}
}

@misc{zhou2020,
  title         = {Unbiased Measurement of Feature Importance in Tree-Based Methods},
  author        = {Zhengze Zhou and Giles Hooker},
  year          = {2020},
  eprint        = {1903.05179},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1903.05179}
}

@misc{model_agnostic_interp,
  title         = {Model-Agnostic Interpretability of Machine Learning},
  author        = {Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
  year          = {2016},
  eprint        = {1606.05386},
  archiveprefix = {arXiv},
  primaryclass  = {stat.ML},
  url           = {https://arxiv.org/abs/1606.05386}
}

@article{Breiman1996HeuristicsOI,
  title   = {Heuristics of instability and stabilization in model selection},
  author  = {L. Breiman},
  journal = {Annals of Statistics},
  year    = {1996},
  volume  = {24},
  pages   = {2350-2383},
  url     = {https://api.semanticscholar.org/CorpusID:50814231}
}