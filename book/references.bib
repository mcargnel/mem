@ARTICLE{natekin_knoll_2013,

AUTHOR={Natekin, Alexey  and Knoll, Alois },

TITLE={Gradient boosting machines, a tutorial},

JOURNAL={Frontiers in Neurorobotics},

VOLUME={7},

YEAR={2013},

URL={https://www.frontiersin.org/journals/neurorobotics/articles/10.3389/fnbot.2013.00021},

DOI={10.3389/fnbot.2013.00021},

ISSN={1662-5218},

ABSTRACT={<p>Gradient boosting machines are a family of powerful machine-learning techniques that have shown considerable success in a wide range of practical applications. They are highly customizable to the particular needs of the application, like being learned with respect to different loss functions. This article gives a tutorial introduction into the methodology of gradient boosting methods with a strong focus on machine learning aspects of modeling. A theoretical information is complemented with descriptive examples and illustrations which cover all the stages of the gradient boosting model design. Considerations on handling the model complexity are discussed. Thr
ee practical examples of gradient boosting applications are presented and comprehensively analyzed.</p>}}
@misc{fisher2019,
      title={All Models are Wrong, but Many are Useful: Learning a Variable's Importance by Studying an Entire Class of Prediction Models Simultaneously}, 
      author={Aaron Fisher and Cynthia Rudin and Francesca Dominici},
      year={2019},
      eprint={1801.01489},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/1801.01489}, 
}
@misc{mentch_2021,
      title={Bridging Breiman's Brook: From Algorithmic Modeling to Statistical Learning}, 
      author={Lucas Mentch and Giles Hooker},
      year={2021},
      eprint={2102.12328},
      archivePrefix={arXiv},
      primaryClass={stat.OT},
      url={https://arxiv.org/abs/2102.12328}, 
}


@article{breiman_2001,
author = {Leo Breiman},
title = {{Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)}},
volume = {16},
journal = {Statistical Science},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {199 -- 231},
year = {2001},
doi = {10.1214/ss/1009213726},
URL = {https://doi.org/10.1214/ss/1009213726}
}


@article{freiesleben_2024,
author = {Freiesleben, Timo and König, Gunnar and Molnar, Christoph and Tejero-Cantero, Álvaro},
year = {2024},
month = {07},
pages = {},
title = {Scientific Inference with Interpretable Machine Learning: Analyzing Models to Learn About Real-World Phenomena},
volume = {34},
journal = {Minds and Machines},
doi = {10.1007/s11023-024-09691-z}
}

@article{friedman_2001,
author = {Jerome H. Friedman},
title = {{Greedy function approximation: A gradient boosting machine.}},
volume = {29},
journal = {The Annals of Statistics},
number = {5},
publisher = {Institute of Mathematical Statistics},
pages = {1189 -- 1232},
keywords = {boosting, decision trees, Function estimation, robust nonparametric regression},
year = {2001},
doi = {10.1214/aos/1013203451},
URL = {https://doi.org/10.1214/aos/1013203451}
}


@book{CART,
	author = {Leo Breiman and Jerome H. Friedman and Richard A. Olshen and Charles J. Stone},
	title = {Classification and Regression Trees},
	year = {1984},
	publisher = {Routledge},
	doi = {10.1201/9781315139470}
	
}

@book{ISLP,
	title = {An Introduction to Statistical Learning: with Applications in R},
	author = {Gareth James and Daniela Witten and Trevor Hastie and Robert Tibshirani},
	year = {2023},
	publisher = {Springer},
	doi = {10.1007/978-3-031-38747-0}
}

@book{ESL,
	title = {The Elements of Statistical Learning},
	author = {Trevor Hastie and Robert Tibshirani and Jerome Friedman},
	year = {2009},
	publisher = {Springer},
	doi = {10.1007/978-0-387-84858-7}
}
@article{intro_ensemble_methods,
	author = {Berk, Richard},
	year = {2006},
	month = {02},
	pages = {},
	title = {An Introduction to Ensemble Methods for Data Analysis},
	volume = {34},
	journal = {Sociological Methods & Research},
	doi = {10.1177/0049124105283119}
}

@misc{model_agnostic_interp,
	title={Model-Agnostic Interpretability of Machine Learning}, 
	author={Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin},
	year={2016},
	eprint={1606.05386},
	archivePrefix={arXiv},
	primaryClass={stat.ML},
	url={https://arxiv.org/abs/1606.05386}, 
}
@article{to_explain_or_to_predict,
	author = {Galit Shmueli},
	title = {{To Explain or to Predict?}},
	volume = {25},
	journal = {Statistical Science},
	number = {3},
	publisher = {Institute of Mathematical Statistics},
	pages = {289 -- 310},
	keywords = {causality, data mining, Explanatory modeling, predictive modeling, predictive power, scientific research, statistical strategy},
	year = {2010},
	doi = {10.1214/10-STS330},
	URL = {https://doi.org/10.1214/10-STS330}
}

@article{bagging_predictors,
	author = {Leo Breiman},
	title = {Bagging predictors},
	journaltitle = {Machine Learning},
	date = {1996-08-01}
}

@article{breiman2001a,
	author       = {Leo Breiman},
	title        = {Random Forests},
	journal      = {Machine Learning},
	year         = {2001},
	volume       = {45},
	number       = {1},
	pages        = {5--32},
	doi          = {10.1023/A:1010933404324},
	url          = {https://doi.org/10.1023/A:1010933404324},
	abstract     = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	issn         = {1573-0565}
}


@book{efron1994boostrap,
	title={An Introduction to the Bootstrap},
	author={Efron, B. and Tibshirani, R.J.},
	isbn={9780412042317},
	lccn={93004489},
	series={Chapman \& Hall/CRC Monographs on Statistics \& Applied Probability},
	url={https://books.google.com.ar/books?id=gLlpIUxRntoC},
	year={1994},
	publisher={Taylor \& Francis}
}


@article{Breiman1996HeuristicsOI,
	title={Heuristics of instability and stabilization in model selection},
	author={L. Breiman},
	journal={Annals of Statistics},
	year={1996},
	volume={24},
	pages={2350-2383},
	url={https://api.semanticscholar.org/CorpusID:50814231}
}

@misc{zhou2020,
      title={Unbiased Measurement of Feature Importance in Tree-Based Methods}, 
      author={Zhengze Zhou and Giles Hooker},
      year={2020},
      eprint={1903.05179},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/1903.05179}, 
}


@article{strobl_2008,
author = {Strobl, Carolin and Boulesteix, Anne-Laure and Kneib, Thomas and Augustin, Thomas and Zeileis, Achim},
year = {2008},
month = {08},
pages = {307},
title = {Conditional Variable Importance for Random Forests},
volume = {9},
journal = {BMC bioinformatics},
doi = {10.1186/1471-2105-9-307}
}