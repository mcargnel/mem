Tal como se mencionó anteriormente, uno de los \textit{trade-off} de los ensambles de árboles es que al agrupar múltiples árboles el modelo en conjunto deja de ser interpretable. Esto puede no suponer un problema si el objetivo es predecir, pero si el objetivo es interpretar el modelo el hecho de trabajar con 'cajas negras' puede ser un problema.

Es por ello que se desarrollaron técnicas para extraer información de modelos de aprendizaje automático. Estas técnicas se aplican una vez entrenado el modelo y permiten entender cómo el mismo utiliza las variables. Pese a que existen distintas técnicas para interpretar modelos, algunas son específicas para ciertos tipos de modelos mientras que otras son agnósticas. En este trabajo se estudiará como acceder a la importancia de cada covariable y al efecto que tiene cada una de ellas sobre la variable dependiente.

\section{Importancia de las variables}

Con el acceso a información que se tiene actualmente, no es extraño querer incluir múltiples covariables en un mismo modelo. El problema, es que entender cuales de ellas realmente están aportando al mismo no es trivial. Por lo que se desarrollaron múltiples técnicas para evaluarlo, todas con sus argumentos a favor y en contra.

En este trabajo nos centramos en dos: \textit{split gain feature importance} y \textit{permutation feature importance}.

\subsection{\textit{Split Gain Feature Importance}}

Esta técnica fue propuesta por \cite{friedman_2001} y asigna importancias en base a la cantidad de \textit{splits} que cada variable tiene en el árbol. Esta metodología solamente se puede aplicar para árboles de decisión y más formalmente se ve para un solo árbol:
 
\begin{equation}
    \label{eq:split-importance}
\hat{I}_j^{2}(T)=\sum_{t=1}^{J-1}\hat{i}_t^{2}1(v_t=j)    
\end{equation}

donde $I$ es la influencia para el nodo $j$ del árbol $T$, siendo $v$ la \textit{splitting variable} que se suma por todos los nodos no terminales.

La ecuación \autoref{eq:split-importance} es calculada para un solo árbol. Por lo que cuando se trabaja con ensambles de árboles se debe generalizar a

\begin{equation}
\hat{\mathbf{I}}_j^{2}=\frac{1}{B}\sum_{m=1}^{B}\hat{I}^{2}_j(T_b)    
\end{equation}

Intuitivamente, se puede entender como el promedio de los \textit{splits} de cada variable. Sin embargo, esta medida esta basada en heurísticas y no contiene fundamentos estadísticos sólidos. Tal es así que fue criticada por \cite{strobl_2008} por dar demasiada influencia a variables correlacionadas y favorecer variables categóricas con muchas categorias.

El problema de las variables correlacionadas radica en que el modelo no puede distinguir entre aquellas variables que realmente aportan a la generación de los datos con las que simplemente correlacionan con variables influyentes, por lo que termina asignando mayor influencia a variables que no deberían tenerla.

En el caso de las variables categóricas, este método prioriza aquellas variables que son propensas a tener muchos \textit{splits}, sin que esto signifique que las variables son importantes o estan reduciendo el error.

\subsection{Permutation Feature Importance} 

Otra alternativa para calcular las importancias de las variables es utilizar \textit{Permutation Feature Importance} (PFI), técnica desarrollada por \cite{breiman2001a} exclusivamente para \textit{Random Forest} que luego fue generalizada para cualquier modelo \cite{fischer_2019}. Estas técnicas se basan, intuitivamente, en entender cómo cambia la función de pérdida cuando se permutan las variables. Las variables más importantes serán aquellas que más cambien el error del modelo al ser permutadas.
Como se ve a continuación, si bien \cite{fischer_2019} propone métodos para calcular la importancia en modelos de clasificación, se presentará la adaptación a un modelo de regresión. Por lo que habiendo entrenado un modelo de \textit{random forest} $\hat{f}$ para cada covariable $j \in \{1,2,...,J\}$, se construye un nuevo conjunto de datos $X_j$ permutando las observaciones de la covariable $j$ y dejando el resto de las observaciones fijas. Posteriormente, se entrena un nuevo modelo $\hat{f}_j$ utilizando el nuevo conjunto de datos $X_j$ y se calcula el error \textit{out-of-bag}:

\begin{equation}
E_{OOB}(X_j) = \frac{1}{n} \sum_{i=1}^{n} \left( y_i -  \hat{f}^{(b)}(X_{i,j}) \right)^2    
\end{equation}

Luego se compara el error \textit{out-of-bag} con el error del modelo original y se calcula la diferencia mediante:
\begin{equation}
    PFI(j) = E_{OOB}(X_j) - E_{OOB}(X) ~~ \text{ó} ~~ PFI(j) = \frac{E_{OOB}(X_j)}{E_{OOB}(X)}
\end{equation}

Sin embargo, como se mencionó anteriormente este método para calcular importancias no es generalizable dado que utiliza los errores \textit{out-of-bag}, por lo que un modelo que no utilice esta metodología no podría usarla. Sin embargo, es fácilmente extendible a dichos casos. A continuación se presenta la generalización propuesta en \cite{fischer_2019}.

Los autores se basan en la propuesta de \cite{breiman2001a}, pero deciden llamar a la medida de importancia \textit{Model Reliance} (MR). Por lo que partiendo de un modelo entrenado $\hat{f}$, una variable de respuesta $y$, una función de pérdida $L$ y una matriz de diseño $X$ con covariables $j \in \{1,2,...,J\}$, para cada covariable $j$ se permuta dicha covariable y se calcula la pérdida esperada:

\begin{equation}
L_{perm}(f)= \mathcal{E}[L(f,(Y,X_1^{perm},X_2))]    
\end{equation}

donde se permutan los valores de $X_1$, dejando constantes las demás variables. Luego, se compara la pérdida con el error original:

\begin{equation}
MR^{j}(f)=\frac{L^{j}_{perm}(f)}{L(f)}    
\end{equation}

De esta forma se obtiene cuanto afecta cada covariable a la variable de respuesta.

Para calcular $L_{perm}$ los autores proponen dos alternativas. La primera consiste en realizar una permutación por todas las observaciones:

\begin{equation}
    \hat{e}_{perm}(F)\frac{1}{n(n-1)}\sum_{i=1}^n\sum_{j\neq i}L\{f,y_j,X_{1[i,.]},X_{2[i,.]} \}
\end{equation}

Sin embargo esto puede ser computacionalmente muy pesado, por lo que proponen como alternativa dividir la muestra a la mitad y reemplazar los valores de $X_1$ de la primera mitad con los de la segunda y viceversa:

\begin{equation}
    \begin{aligned}
    \hat{e}_{\text{divide}}(f) &= \frac{1}{2 \lfloor n/2 \rfloor} \sum_{i=1}^{\lfloor n/2 \rfloor} \left[
    L\Bigl(f,\Bigl(y_{[i]},\, X_{1\bigl[i+\lfloor n/2 \rfloor\bigr]},\, X_{2[i]}\Bigr)\Bigr) \right. \\
    &\left. \quad + L\Bigl(f,\Bigl(y_{i+\lfloor n/2 \rfloor},\, X_{1[i]},\, X_{2\bigl[i+\lfloor n/2 \rfloor\bigr]}\Bigr)\Bigr)
    \right]. \\
    \end{aligned}    
\end{equation}


\subsection{Model Class Reliance} 
Un problema común a la hora de calcular importancias es que el modelo que terminamos usando se decide únicamente en función de una métrica de error. Esto no supondría inconvenientes si las importancias de los modelos fueran parecidas. Sin embargo, suele pasar que para muchos modelos con un poder predictivo similar las importancias de las variables cambien. A continuación se presenta un ejemplo en el cual se entrenaron múltiples \textit{Gradient Boosting Machines} con diferentes combinaciones de hiperparámetros, eligiendo las combinaciones que minimicen el error cuadrático medio. Para el elegir dicha combinación se utilizó validación cruzada con $k=5$. El conjunto de datos para este ejemplo en particular fue \textit{Abalone} \cite{abalone_1} donde se busca predecir la edad del Abalone en función de distintas características.

Específicamente, para este ejemplo se entrenaron modelos con cantidad de árboles de 100 a 900 (con incrementos de 100), tasas de aprendizaje de 0.01, 0.05 y 0.1, profundidad máxima de 2, 3, 4 y 5, y funciones de pérdida de tipo pérdida cuadrática, error absoluto y Huber. Siendo un total de 324 modelos entrenados. En la \autoref{tbl:display-importances} se pueden ver los 15 modelos que menor error tuvieron, junto con los hiperparámetros asociados. Puede verse claramente, como hay múltiples modelos que tienen una performance predictiva similar pero distintos hiperparámetros.

\begin{table}[ht!]
    \centering
    \caption{Tabla con resultados de los 15 modelos con menor error de predicciones, junto con los parámetros seleccionados.}
    \label{tbl:display-importances}
    \begin{tabular}{lccccc}
    \hline
    Modelo & RMSE & Shrinkage & Función de Pérdida & Profundidad máx & Árboles \\
    \hline
    RF1 & 2.1488 & 0.10 & Huber & 3 & 200 \\
    RF2 & 2.1498 & 0.05 & Huber & 4 & 200 \\
    RF3 & 2.1498 & 0.01 & Huber & 4 & 900 \\
    RF4 & 2.1506 & 0.10 & Huber & 4 & 100 \\
    RF5 & 2.1513 & 0.01 & Pérdida cuadrática & 4 & 600 \\
    RF6 & 2.1516 & 0.05 & Pérdida cuadrática & 4 & 200 \\
    RF7 & 2.1518 & 0.05 & Huber & 3 & 300 \\
    RF8 & 2.1518 & 0.01 & Pérdida cuadrática & 4 & 700 \\
    RF9 & 2.1519 & 0.05 & Huber & 3 & 400 \\
    RF10 & 2.1522 & 0.01 & Pérdida cuadrática & 4 & 800 \\
    RF11 & 2.1522 & 0.01 & Huber & 4 & 800 \\
    RF12 & 2.1526 & 0.05 & Pérdida cuadrática & 4 & 100 \\
    RF13 & 2.1530 & 0.01 & Pérdida cuadrática & 4 & 500 \\
    RF14 & 2.1531 & 0.10 & Pérdida cuadrática & 4 & 100 \\
    RF15 & 2.1537 & 0.01 & Pérdida cuadrática & 4 & 900 \\
    \hline
    \end{tabular}
\end{table}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\textwidth]{images/capitulo_4/feature_importances.pdf}
    \caption{Importancias de las variables para distintos modelos de Gradient Boosting con rendimiento similar.}
    \label{fig:line-plot-importances}
\end{figure}

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\textwidth]{images/capitulo_4/importance_heatmap.pdf}
    \caption{Mapa de calor de las importancias de las variables para distintos modelos de Gradient Boosting con rendimiento similar.}
    \label{fig:heatmap-importances}
\end{figure}


Además, y posiblemente más preocupante, cuando vemos la Figura \autoref{fig:line-plot-importances} vemos como las importancias de las variables (calculadas mediante \textit{Permutation Feature Importance}) varían entre modelos con un poder predictivo similar. Este mismo fenómeno puede verse también en la Figura \autoref{fig:heatmap-importances} donde para los distintos modelos las variables tienen rankings de importancia muy diferentes. El heatmap muestra claramente cómo una misma variable puede ser considerada la más importante (ranking 1) en un modelo, mientras que en otro puede tener una importancia mucho menor (ranking 5 o 6). Esta inconsistencia en la importancia de las variables entre modelos con rendimiento similar sugiere inestabilidad en la interpretación de los resultados a los que se puede llegar.

El fenómeno observado, donde múltiples modelos con similar capacidad predictiva asignan importancias sustancialmente diferentes a las variables, se conoce como el \textit{Rashomon Effect} en estadística \cite{breiman_2001}. Este término, inspirado en la película de Kurosawa donde un mismo evento es narrado desde perspectivas contradictorias es formalizado en \cite{fischer_2019} como el "conjunto de Rashomon", definido como la colección de modelos con rendimiento predictivo similar pero con diferentes estructuras internas. Esta multiplicidad de explicaciones válidas representa un desafío fundamental para la interpretabilidad en \textit{machine learning}, ya que sugiere que la importancia de una variable no es una propiedad intrínseca de los datos, sino que depende del modelo específico elegido.

Es por ello que los autores trabajaron sobre la \textit{Model Class Reliance} (MCR), que se basa en para una clase de modelos $\mathcal{F}$, MCR es el rango de todos los posibles valores de MR sobre modelos que tengan un poder predictivo similar al óptimo. Para lo cual, primero se define el "conjunto de Rashomon" como

\begin{equation}
R(\epsilon) = \{f \in \mathcal{F} \mid e_{orig}(f) \le e_{orig}(f_{ref}) + \epsilon \}    
\end{equation}

Este set contiene todos los modes cuya pérdida sea como mucho $\epsilon$ peor que el modelo que minimiza el error $f_{ref}$

Luego se define el intervalo MCR como

\begin{equation}
[MCR^{-}(\epsilon), MCR^{+}(\epsilon)]=[\min_{f\in R(\epsilon)}MR(f), \max_{f\in R(\epsilon)}MR(f)]    
\end{equation}

Este intervalo MCR proporciona información valiosa sobre la importancia de una variable en toda la clase de modelos.

\subsubsection{Interpretación}
Si $MCR^{-}(\epsilon)$ es mayor que 1, significa que todos los modelos "buenos" (dentro del conjunto de Rashomon) dependen de la variable más que del modelo de referencia, lo que sugiere que la variable es consistentemente importante. Por otro lado, si $MCR^{+}(\epsilon)$ es cercano a 1, indica que ningún modelo "bueno" depende fuertemente de esa variable.

La interpretación del MCR nos permite evaluar si la importancia de una variable es consistente a través de diferentes modelos o si es específica de una arquitectura particular. Un intervalo MCR estrecho sugiere que la importancia de la variable es estable en todos los modelos con buen rendimiento, mientras que un intervalo amplio indica que la importancia varía significativamente dependiendo del modelo elegido. Por ejemplo, pueden ocurrir casos donde para una variable $J$ el $MCR^{-}(\epsilon)$ sea cercano a 1 pero $MCR^{+}(\epsilon)$ no. Esto indicaría existen modelos con buena capacidad predictiva que no dependen de la variable $J$ por lo que dicha variable no sería tan importante.

\subsubsection{Estimación}
En la práctica para estimar MCR se utiliza un estimador \textit{plug-in}, es decir se calcula $\hat{MR}(f)$ para todos los modelos que integren el conjunto Rashomon empírico $\hat{R}(\epsilon)$.

\section{Efecto de las variables} 
Otra técnica útil para interpretar modelos de árboles es mediante \textit{Partial Dependence Plots}. Los mismos también fueron introducidos por \cite{friedman_2001}, donde los plantea como una técnica para aislar el efecto de una covariable sobre la variable de respuesta, más formalmente lo que se busca estimar es:

\begin{equation}
    \label{eq:pdp}
\bar{f}(X_z)=\mathcal{E}_{x_l}[\hat{f}(x)]=\int \hat{F}(z,x_l)p(x_l)dx_l    
\end{equation}

donde $\hat{f}(x)$ es el modelo entrenado, $z$ es el subset de variables a las que queremos estimar el efecto, $x_l$ representa todas las otras predicciones (que se mantienen constantes), $p(x_l)$ es la distribución conjunta de todas las otras variables y $\mathcal{E}_{x_l}[.]$ es la esperanza sobre la distribución $x_l$

Sin embargo, en la práctica es raro conocer $p(x_l)$ por lo que se utiliza el siguiente procedimiento:
 
\begin{equation}
\bar{f}(z)\approx\frac{1}{n}\sum_{i=1}^{n}\hat{f}(z,x_l^{i})
\end{equation}

Cuyo algoritmo sería, para cada observación $i$ en la muestra, tomar los valores originales de las variables no seleccionadas $x_l^{i}$. Posteriormente, se utilizan los valores modificados de las variables z en cada $x_l^{i}$ dentro del modelo $\hat{F}$ para obtener las predicciones $\hat F(z,x_l^{i})$. Finalmente, se promedian las predicciones para $i=1,2,...,n$.

Cabe aclarar que al modificar solamente una variable y dejar el resto como estaban, se esta asumiendo que las variables $z$ son independientes a todo el resto. Algo que puede o no ser cierto dependiendo de la naturaleza de los datos. Además, al cambiar los valores de las variables a evaluar se podrían dar combinaciones poco realistas en la practica.

Con el fin de ejemplificar los gráficos de dependencia parcial se entrenó un modelo de \textit{Gradient Boosting Machine} con 100 árboles con el fin de predecir la edad del \textit{Abalone} en función de distintas medidas físicas. Una vez entrenado el modelo, se puede ver en la \autoref{fig:partial-dependence-plot} el efecto del peso total en la edad.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\textwidth]{images/capitulo_4/partial_dependence_plot.pdf}
    \caption{Gráfico de dependencia parcial para el peso total en la predicción de la edad del Abalone.}
    \label{fig:partial-dependence-plot}
\end{figure}


Sin embargo, como se mencionó anteriormente, los PDPs sufren problemas cuando las variables no son independientes. Para solventar esto, se puede utilizar el método de \textit{Individual Conditional Expectation} (ICE) que consiste en calcular el PDP para cada observación en la muestra.

\subsection{\textit{Individual Conditional Expectation }(ICE)}

Los ICE fueron introducidos por \cite{Goldstein2015} como una técnica para estimar el efecto de una covariable sobre la variable de respuesta, más formalmente lo que se busca estimar es:

\begin{equation}
f_i(x_z)=E[Y|X_z=x_z,X_{-z}=x_{-z}^{i}]    
\end{equation}

Es decir, en lugar de calcular el efecto promedio de $z$ sobre $y$ calculando el promedio de $f(z,x_{-z})$ para todas las observaciones, se calcula el efecto de $z$ para cada observación $i$ manteniendo el resto de las variables constantes. De esta manera se puede obtener una estimación del efecto de $z$ para cada observación y no solo un promedio, teniendo así una visión más detallada.

De esta forma, se podrían entender los PDPs como el promedio de los ICE. Dado que mientras el primero se calcula mediante la distribución marginal de las variables, los segundos lo hacen utilizando la distribución condicional.

Cabe destacar que PDP y ICE deberían ser similares cuando las variables son independientes, pero cuando las variables no son independientes, los ICE pueden variar mucho por lo que se recomienda utilizar las dos técnicas.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\textwidth]{images/capitulo_4/ice_plot.pdf}
    \caption{Gráfico de \textit{Individual Conditional Expectation} (ICE) para el peso total en la predicción de la edad del Abalone.}
    \label{fig:ice-plot}
\end{figure}

Como se puede ver en la \autoref{fig:ice-plot}, los ICE son mucho más variables que los PDP, lo que permite tener una visión más detallada del efecto de la variable en cuestión. Sin embargo, se puede ver que los ICE tienen distintas ordenadas al origen, lo que dificulta la comparación entre distintas variables y la identificación de efectos heterogéneos.

Para solventar este problema, se puede utilizar el método de \textit{Centered ICE} que consiste en centrar los ICE en algún punto de la distribución de la variable (los autores de este método sugieren el valor mínimo o máximo de la variable para tener gráficos sencillos de interpretar). Más formalmente se busca centrar cada curva $\hat{f}_i$ en un valor de referencia $(x^*,x_{ci})$ tal que:

\begin{equation}
\hat{f}_{cent}^{(i)}=\hat{f}^{(i)}-1\hat{f}(x^*,x_{ci})
\end{equation}

A modo de ejemplo, si $x^*$ es el valor mínimo de $x_s$ todas las curvas empiezan en cero, por otro lado si es el máximo se puede ver ver el efecto acumulado de $x_s$ sobre $\hat{f}$ relativo al caso base.

A modo de ejemplo, en la \autoref{fig:centered-ice-plot} se puede ver el \textit{Centered ICE} para la misma variable de antes. En este caso $x^*$ es el valor mínimo por lo que todas las curvas empiezan en cero. También, se puede ver en rojo la curva de PDP centrada.

\begin{figure}[ht!]
    \centering
    \includegraphics[width=1\textwidth]{images/capitulo_4/centered_ice_plot.pdf}
    \caption{Gráfico de \textit{Individual Conditional Expectation} (ICE) centrado para el peso total en la predicción de la edad del Abalone.}
    \label{fig:centered-ice-plot}
\end{figure}


\subsection{\textit{Derivative ICE}}
Con el fin de seguir identificando las interacciones entre las variables los autores proponen calcular los gráficos de la derivada parcial de $\hat{f}$ con respecto a $x_s$. A modo de formalizarlo, se asume independencia entre $x_s$ y el resto de las covariables, por lo que $\hat{f}$ puede ser escrita como:

\begin{equation}
\hat{f}(x)=\hat{f}(x_s,x_c)=g(x_s)+h(x_c) \Rightarrow \frac{\partial \hat{f}(x)}{\partial x_s}=g'(x_s)    
\end{equation}

lo cual significa que la relación entre $x_s$ y $\hat{f}$ no depende de $x_c$. Cuando esto se cumple todas las líneas deberían ser equivalentes.

\subsubsection{Causalidad}

Pese a que \cite{pearl_2018} menciona por qué los modelos de aprendizaje automático no pueden ser utilizados para hacer inferencia causal, dado que todos estos modelos se basan en el nivel de asociación entre variables, sin llegar a la intervención o contrafactuales. Sin embargo, \cite{Zhao_2021} plantea que los PDPs y ICEs bajo ciertas condiciones pueden tener una interpretación en causal.

Los autores se basan en que \autoref{eq:pdp} es igual al \textit{backdoor adjustment} definido por \cite{Pearl1993} como
\begin{equation}
    \label{eq:back-door}
P(Y|do(X_s=x_s))=\int P(Y|X_s=x_s, X_c=x_c)dP(x_c)    
\end{equation}

donde $P(Y|do(X_s=x_s))$ se refiere a la distribución de $Y$ luego de hacer un intervención en $X_s$. Esta formula permite identificar el efecto causal de $X_s$ en $Y$ siempre y cuando se cumpla cuando (1) ningún nodo $X_c$ sea dependiente descendiente de $X_s$ y (2) que $X_c$ 'bloquee' cualquier camino "back-door" entre $X_s$ e $Y$ que se puede pensar intuitivamente como una causa común entre $X_s$ e $Y$ que impide entender el efecto causal de $X_s$.

\begin{equation}
E[Y|do(X_s=x_s)]=\int E[Y|X_s=x_s, X_c=x_c]dP(x_c)
\end{equation}

Ahora si partimos de $\hat{f}(x)$ la función aprendida por el modelo, que aproxima la expectativa condicional, es decir,  
\begin{equation}
    \label{eq:back-door-expectation}
    \hat{f}(x) \approx E[Y \mid X=x].    
\end{equation}

asumiendo que el modelo está bien calibrado, es decir,  
\begin{equation}
\hat{f}(x_S, x_C) = E[Y \mid X_S = x_S, X_C = x_C],    
\end{equation}
podemos vincular ambas expresiones de la siguiente forma:  

\begin{equation}
\begin{aligned}
\text{PDP}(x_S) &= \int \hat{f}(x_S, x_C) \, dP(x_C) \\
                &= \int E[Y \mid X_S = x_S, X_C = x_C] \, dP(x_C) \\
                &= E[Y \mid do(X_S = x_S)]\\
\end{aligned}
\end{equation}


por lo que puede verse que \autoref{eq:pdp} y \autoref{eq:back-door-expectation} son equivalentes si condicionando en el set $C$ es complementario al set $S$.

\section{Relación con el proceso generador de los datos} 

Es interesante considerar que, aunque los modelos de aprendizaje automático suelen superar en rendimiento predictivo a los métodos estadísticos convencionales, estos últimos permiten establecer un vínculo directo entre los parámetros del modelo y las propiedades del proceso generador de datos (DGP). Esta capacidad resulta fundamental para comprender en profundidad la naturaleza subyacente de la población.

En \cite{freiesleben_2024} se propone un marco estadístico que relaciona formalmente las técnicas de interpretación basadas en \textit{Partial Dependence Plots} (PDP) y \textit{Permutation Feature Importance} (PFI) con el DGP. En este contexto, se introducen los conceptos de DGP-PD y DGP-PFI, definidos respectivamente como

\begin{equation}
DGP\text{-}PD(x)=E_{X_C}\left[f(x, X_C)\right]
\end{equation}
y

\begin{equation}
DGP\text{-}PFI = E_{\bar{X}_S, X_C, Y}\left[L\left(Y, f(\bar{X}_S, X_C)\right)\right] - E_{X,Y}\left[L\left(Y, f(X)\right)\right]    
\end{equation}

donde $f(x)=E[Y|X=x]$ es la función de esperanza condicional. Los autores realizan una descomposición del error de estos estimadores en términos de sesgo y varianza, identificando fuentes de error tales como la aproximación de Monte Carlo para el cálculo de las esperanzas, la posible especificación incorrecta del modelo y la variabilidad inherente al proceso de entrenamiento.

Para capturar la incertidumbre debida a la variabilidad en el entrenamiento, se introduce la metodología \textit{learner-PD/PFI}, la cual promedia los resultados obtenidos a partir de múltiples modelos reajustados. Esto permite incorporar la variabilidad del aprendizaje en la estimación y, en consecuencia, obtener intervalos de confianza que reflejen de manera más realista la incertidumbre de las medidas de interpretación.

No obstante, esta metodología asume condiciones fuertes, como la insesgadez del modelo. Además, los intervalos de confianza pueden resultar demasiado estrechos debido al solapamiento en el remuestreo utilizado para reentrenar los modelos, y la dependencia entre variables puede afectar los estimadores.
